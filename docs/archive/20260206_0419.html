<!doctype html>
<html><head><meta charset="utf-8"><meta name="viewport" content="width=device-width,initial-scale=1">
<title>arXiv 论文速递</title><style>
:root {
  --bg:#f8fafc; --card:#ffffff; --text:#0f172a; --muted:#667085; --border:#e5e7eb; --acc:#2563eb;
}
:root[data-theme="dark"] {
  --bg:#0b0f17; --card:#111827; --text:#e5e7eb; --muted:#9ca3af; --border:#1f2937; --acc:#2563eb;
}
*{box-sizing:border-box} body{margin:0;background:var(--bg);color:var(--text);
  font-family:ui-sans-serif,system-ui,-apple-system,Segoe UI,Roboto,Helvetica,Arial;line-height:1.6;}
.container{max-width:900px;margin:0 auto;padding:18px;}
.header{display:flex;gap:10px;justify-content:space-between;align-items:center;margin:8px 0 16px;flex-wrap:wrap}
h1{font-size:22px;margin:0}
.badge{font-size:12px;color:#111827;background:var(--acc);padding:2px 8px;border-radius:999px}
.card{background:var(--card);border:1px solid var(--border);border-radius:16px;padding:16px 18px;margin:14px 0;box-shadow:0 1px 2px rgba(0,0,0,.04)}
.title{font-weight:700;margin:0 0 6px 0;font-size:18px}
.meta-line{color:var(--muted);font-size:13px;margin:2px 0}
.links a{color:var(--acc);text-decoration:none;margin-right:12px}
.detail{margin-top:10px;background:rgba(2,6,23,.03);border:1px solid var(--border);border-radius:10px;padding:8px 10px}
summary{cursor:pointer;color:var(--acc)}
.mono{white-space:pre-wrap;background:rgba(2,6,23,.03);border:1px solid var(--border);padding:10px;border-radius:10px}
.row{display:grid;grid-template-columns:1fr;gap:12px}
@media (min-width: 860px) {
  .row-2{grid-template-columns:1fr 1fr}
}
.footer{color:var(--muted);font-size:13px;margin:20px 0 10px}
.hr{height:1px;background:var(--border);margin:14px 0}
.history-list a{display:block;color:var(--acc);text-decoration:none;margin:4px 0}
.controls{display:flex;gap:8px;align-items:center}
.btn{border:1px solid var(--border);background:var(--card);padding:6px 10px;border-radius:10px;cursor:pointer;color:var(--text)}
.btn:hover{border-color:var(--acc)}
</style>
<script>
(function() {
  const root = document.documentElement;
  function apply(t) {
    if (t==='dark') root.setAttribute('data-theme','dark');
    else if (t==='light') root.removeAttribute('data-theme');
    else {
      if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches)
        root.setAttribute('data-theme','dark');
      else root.removeAttribute('data-theme');
    }
  }
  let t = localStorage.getItem('theme') || 'light';
  if (!['light','dark','auto'].includes(t)) t='light';
  apply(t);
  window.__toggleTheme = function() {
    let cur = localStorage.getItem('theme') || 'light';
    if (cur==='light') cur='dark';
    else if (cur==='dark') cur='auto';
    else cur='light';
    localStorage.setItem('theme', cur);
    apply(cur);
    const el=document.getElementById('theme-label');
    if(el) el.textContent = cur.toUpperCase();
  }
  window.__expandAll = function(open) {
    document.querySelectorAll('details').forEach(d => d.open = !!open);
  }
})();
</script>
</head>
<body>
  <div class="container">
    <div class="header">
      <h1>arXiv 论文速递</h1>
      <div style="display:flex;gap:10px;align-items:center">
        
<div class="controls">
  <button class="btn" onclick="__toggleTheme()">Theme: <span id="theme-label" style="margin-left:6px">AUTO</span></button>
  <button class="btn" onclick="__expandAll(true)">Expand All</button>
  <button class="btn" onclick="__expandAll(false)">Collapse All</button>
</div>

        <span class="badge">2026-02-06 04:19</span>
      </div>
    </div>
    <div class="hr"></div>
    <div>Snapshot: 20260206_0419</div>
    <div class="row"><div class="card">
<div class="title">Agentic AI in Healthcare &amp; Medicine: A Seven-Dimensional Taxonomy for Empirical Evaluation of LLM-based Agents</div>
<div class="meta-line">Authors: Shubham Vatsal, Harsh Dubey, Aditi Singh</div>
<div class="meta-line">Venue: IEEE Access, vol. 14, pp. 4840-4863, 2026</div>
<div class="meta-line">First: 2026-02-04T17:59:14+00:00 · Latest: 2026-02-04T17:59:14+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04813v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04813v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine. Reported studies demonstrate competence on various tasks ranging from EHR analysis and differential diagnosis to treatment planning and research workflows. Yet the literature largely consists of overviews which are either broad surveys or narrow dives into a single capability (e.g., memory, planning, reasoning), leaving healthcare work without a common frame. We address this by reviewing 49 studies using a seven-dimensional taxonomy: Cognitive Capabilities, Knowledge Management, Interaction Patterns, Adaptation &amp; Learning, Safety &amp; Ethics, Framework Typology and Core Tasks &amp; Subtasks with 29 operational sub-dimensions. Using explicit inclusion and exclusion criteria and a labeling rubric (Fully Implemented, Partially Implemented, Not Implemented), we map each study to the taxonomy and report quantitative summaries of capability prevalence and co-occurrence patterns. Our empirical analysis surfaces clear asymmetries. For instance, the External Knowledge Integration sub-dimension under Knowledge Management is commonly realized (~76% Fully Implemented) whereas Event-Triggered Activation sub-dimenison under Interaction Patterns is largely absent (~92% Not Implemented) and Drift Detection &amp; Mitigation sub-dimension under Adaptation &amp; Learning is rare (~98% Not Implemented). Architecturally, Multi-Agent Design sub-dimension under Framework Typology is the dominant pattern (~82% Fully Implemented) while orchestration layers remain mostly partial. Across Core Tasks &amp; Subtasks, information centric capabilities lead e.g., Medical Question Answering &amp; Decision Support and Benchmarking &amp; Simulation, while action and discovery oriented areas such as Treatment Planning &amp; Prescription still show substantial gaps (~59% Not Implemented).</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large Language Model (LLM)-based agents that plan, use tools and act has begun to shape healthcare and medicine.</div>
</details>
</div>
<div class="card">
<div class="title">Supporting software engineering tasks with agentic AI: Demonstration on document retrieval and test scenario generation</div>
<div class="meta-line">Authors: Marian Kica, Lukas Radosky, David Slivka, Karin Kubinova, Daniel Dovhun, Tomas Uhercik, Erik Bircak, Ivan Polasek</div>
<div class="meta-line">First: 2026-02-04T16:33:16+00:00 · Latest: 2026-02-04T16:33:16+00:00</div>
<div class="meta-line">Comments: This is a preprint of a paper that was accepted at the International Conference on Artificial Intelligence, Computer, Data Sciences and Applications (ACDSA 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04726v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04726v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The introduction of large language models ignited great retooling and rethinking of the software development models. The ensuing response of software engineering research yielded a massive body of tools and approaches. In this paper, we join the hassle by introducing agentic AI solutions for two tasks. First, we developed a solution for automatic test scenario generation from a detailed requirements description. This approach relies on specialized worker agents forming a star topology with the supervisor agent in the middle. We demonstrate its capabilities on a real-world example. Second, we developed an agentic AI solution for the document retrieval task in the context of software engineering documents. Our solution enables performing various use cases on a body of documents related to the development of a single software, including search, question answering, tracking changes, and large document summarization. In this case, each use case is handled by a dedicated LLM-based agent, which performs all subtasks related to the corresponding use case. We conclude by hinting at the future perspectives of our line of research.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The introduction of large language models ignited great retooling and rethinking of the software development models.</div>
</details>
</div>
<div class="card">
<div class="title">EvoFSM: Controllable Self-Evolution for Deep Research with Finite State Machines</div>
<div class="meta-line">Authors: Shuo Zhang, Chaofa Yuan, Ryan Guo, Xiaomin Yu, Rui Xu, Zhangquan Chen, Zinuo Li, Zhi Yang, Shuhao Guan, Zhenheng Tang, Sen Hu, Liwen Zhang, Ronghao Chen, Huacan Wang</div>
<div class="meta-line">First: 2026-01-14T13:19:13+00:00 · Latest: 2026-02-04T15:14:19+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.09465v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.09465v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries. Recent work therefore explores self-evolution by allowing agents to rewrite their own code or prompts to improve problem-solving ability, but unconstrained optimization often triggers instability, hallucinations, and instruction drift. We propose EvoFSM, a structured self-evolving framework that achieves both adaptability and control by evolving an explicit Finite State Machine (FSM) instead of relying on free-form rewriting. EvoFSM decouples the optimization space into macroscopic Flow (state-transition logic) and microscopic Skill (state-specific behaviors), enabling targeted improvements under clear behavioral boundaries. Guided by a critic mechanism, EvoFSM refines the FSM through a small set of constrained operations, and further incorporates a self-evolving memory that distills successful trajectories as reusable priors and failure patterns as constraints for future queries. Extensive evaluations on five multi-hop QA benchmarks demonstrate the effectiveness of EvoFSM. In particular, EvoFSM reaches 58.0% accuracy on the DeepSearch benchmark. Additional results on interactive decision-making tasks further validate its generalization.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>EvoFSM：基于有限状态机的可控自进化深度研究框架</div>
<div class="mono" style="margin-top:8px">尽管基于大语言模型（LLM）的智能体在深度研究中展现出潜力，但现有方法多依赖固定工作流，难以适应现实世界的开放性问题。近期研究尝试通过让智能体重写自身代码或提示来实现自我进化以提升问题解决能力，但无约束优化常导致不稳定、幻觉现象及指令漂移。为此，我们提出EvoFSM——一种结构化自进化框架，通过演化显式有限状态机（FSM）而非自由形式重写，兼顾适应性与可控性。EvoFSM将优化空间解耦为宏观流程（状态转移逻辑）和微观技能（状态特定行为），在清晰行为边界下实现精准改进。在评审机制引导下，EvoFSM通过有限约束操作优化FSM，并引入自进化记忆模块：将成功轨迹提炼为可复用先验知识，失败模式转化为未来查询的约束条件。在五个多跳问答基准测试上的广泛评估验证了EvoFSM的有效性，其在DeepSearch基准测试中达到58.0%准确率。交互式决策任务的附加结果进一步证实了其泛化能力。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">While LLM-based agents have shown promise for deep research, most existing approaches rely on fixed workflows that struggle to adapt to real-world, open-ended queries.</div>
</details>
</div>
<div class="card">
<div class="title">VILLAIN at AVerImaTeC: Verifying Image-Text Claims via Multi-Agent Collaboration</div>
<div class="meta-line">Authors: Jaeyoon Jung, Yejun Yoon, Seunghyun Yoon, Kunwoo Park</div>
<div class="meta-line">First: 2026-02-04T14:12:55+00:00 · Latest: 2026-02-04T14:12:55+00:00</div>
<div class="meta-line">Comments: A system description paper for the AVerImaTeC shared task at the Ninth FEVER Workshop (co-located with EACL 2026)</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04587v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04587v1">PDF</a> · <a href="https://github.com/ssu-humane/VILLAIN">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">This paper describes VILLAIN, a multimodal fact-checking system that verifies image-text claims through prompt-based multi-agent collaboration. For the AVerImaTeC shared task, VILLAIN employs vision-language model agents across multiple stages of fact-checking. Textual and visual evidence is retrieved from the knowledge store enriched through additional web collection. To identify key information and address inconsistencies among evidence items, modality-specific and cross-modal agents generate analysis reports. In the subsequent stage, question-answer pairs are produced based on these reports. Finally, the Verdict Prediction agent produces the verification outcome based on the image-text claim and the generated question-answer pairs. Our system ranked first on the leaderboard across all evaluation metrics. The source code is publicly available at https://github.com/ssu-humane/VILLAIN.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to verify multimodal claims combining images and text, this paper presents VILLAIN, a system employing prompt-based multi-agent collaboration for fact-checking. The method involves retrieving textual and visual evidence from an enriched knowledge store, deploying modality-specific and cross-modal agents to analyze evidence and generate reports, creating question-answer pairs from these reports, and finally using a Verdict Prediction agent to determine the claim&#x27;s veracity. Experimentally, VILLAIN achieved first place across all evaluation metrics in the AVerImaTeC shared task.</div>
<div class="mono" style="margin-top:8px">针对验证图文混合声明的需求，本文提出VILLAIN系统，通过基于提示的多智能体协作进行事实核查。该方法从增强的知识库中检索文本和视觉证据，部署模态专用和跨模态智能体分析证据并生成报告，基于报告创建问答对，最终由裁决预测智能体判定声明真实性。在AVerImaTeC评测任务中，该系统在所有评估指标上均排名第一。</div>
</details>
</div>
<div class="card">
<div class="title">Provable Target Sample Complexity Improvements as Pre-Trained Models Scale</div>
<div class="meta-line">Authors: Kazuto Fukuchi, Ryuichiro Hataya, Kota Matsui</div>
<div class="meta-line">First: 2026-02-04T05:51:56+00:00 · Latest: 2026-02-04T05:51:56+00:00</div>
<div class="meta-line">Comments: AISTATS2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04233v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04233v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Pre-trained models have become indispensable for efficiently building models across a broad spectrum of downstream tasks. The advantages of pre-trained models have been highlighted by empirical studies on scaling laws, which demonstrate that larger pre-trained models can significantly reduce the sample complexity of downstream learning. However, existing theoretical investigations of pre-trained models lack the capability to explain this phenomenon. In this paper, we provide a theoretical investigation by introducing a novel framework, caulking, inspired by parameter-efficient fine-tuning (PEFT) methods such as adapter-based fine-tuning, low-rank adaptation, and partial fine-tuning. Our analysis establishes that improved pre-trained models provably decrease the sample complexity of downstream tasks, thereby offering theoretical justification for the empirically observed scaling laws relating pre-trained model size to downstream performance, a relationship not covered by existing results.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Pre-trained models have become indispensable for efficiently building models across a broad spectrum of downstream tasks.</div>
<div class="mono" style="margin-top:8px">本研究受实证观察启发：更大的预训练模型能降低下游任务样本复杂度，但该现象缺乏理论解释。为此，论文提出名为&#x27;填隙&#x27;的新分析框架，其方法受参数高效微调（PEFT）技术（如适配器、低秩适应）的启发，从理论层面分析预训练模型对下游学习的影响。核心实验结果表明，改进的（如更大或更高质量的）预训练模型可严格证明降低下游学习所需样本复杂度，从而首次为预训练模型规模与下游性能间的经验缩放定律提供了理论依据。</div>
</details>
</div>
<div class="card">
<div class="title">From Helpfulness to Toxic Proactivity: Diagnosing Behavioral Misalignment in LLM Agents</div>
<div class="meta-line">Authors: Xinyue Wang, Yuanhe Zhang, Zhengshuo Gong, Haoran Gao, Fanyu Meng, Zhenhong Zhou, Li Sun, Yang Liu, Sen Su</div>
<div class="meta-line">First: 2026-02-04T04:29:04+00:00 · Latest: 2026-02-04T04:29:04+00:00</div>
<div class="meta-line">Comments: 9 pages (excluding appendices), 6 figures. Code is available at https://github.com/wxyoio-0715/Toxic-Proactivity</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04197v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04197v1">PDF</a> · <a href="https://github.com/wxyoio-0715/Toxic-Proactivity">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The enhanced capabilities of LLM-based agents come with an emergency for model planning and tool-use abilities. Attributing to helpful-harmless trade-off from LLM alignment, agents typically also inherit the flaw of &quot;over-refusal&quot;, which is a passive failure mode. However, the proactive planning and action capabilities of agents introduce another crucial danger on the other side of the trade-off. This phenomenon we term &quot;Toxic Proactivity&#x27;&#x27;: an active failure mode in which an agent, driven by the optimization for Machiavellian helpfulness, disregards ethical constraints to maximize utility. Unlike over-refusal, Toxic Proactivity manifests as the agent taking excessive or manipulative measures to ensure its &quot;usefulness&#x27;&#x27; is maintained. Existing research pays little attention to identifying this behavior, as it often lacks the subtle context required for such strategies to unfold. To reveal this risk, we introduce a novel evaluation framework based on dilemma-driven interactions between dual models, enabling the simulation and analysis of agent behavior over multi-step behavioral trajectories. Through extensive experiments with mainstream LLMs, we demonstrate that Toxic Proactivity is a widespread behavioral phenomenon and reveal two major tendencies. We further present a systematic benchmark for evaluating Toxic Proactive behavior across contextual settings.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The enhanced capabilities of LLM-based agents come with an emergency for model planning and tool-use abilities.</div>
</details>
</div>
<div class="card">
<div class="title">Understanding and Guiding Layer Placement in Parameter-Efficient Fine-Tuning of Large Language Models</div>
<div class="meta-line">Authors: Yichen Xu, Yuyang Liang, Shan Dai, Tianyang Hu, Tsz Nam Chan, Chenhao Ma</div>
<div class="meta-line">First: 2026-02-03T21:05:55+00:00 · Latest: 2026-02-03T21:05:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.04019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.04019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation. Constraints from inference latency in scalable serving and fine-tuning cost in edge or rapid-deployment settings make the choice of which layers to fine-tune unavoidable. Yet current practice typically applies PEFT uniformly across all layers, with limited understanding or leverage of layer selection. This paper develops a unified projected residual view of PEFT on top of a frozen base model. Under a local quadratic approximation, layerwise adaptation is governed by three quantities: (i) the projected residual norm (resnorm), which measures how much correctable bias a layer can capture; (ii) the activation energy, which determines feature conditioning; and (iii) layer coupling, which quantifies how strongly residuals interact across layers. We show that, for squared loss and linear adapters, the resnorm equals a normalized gradient norm, activation energy controls ill-conditioning and noise amplification, and weak coupling yields approximately additive layerwise contributions. Building on these insights, we introduce the Layer Card, a reusable diagnostic that summarizes residual signal strength, compute cost, and performance for each layer of a given model. With an identical model and LoRA configuration, Layer Card-guided placement refines the choice of adapted layers to flexibly prioritize different objectives, such as maximizing performance or reducing fine-tuning cost. Moreover, on Qwen3-8B, we show that selectively adapting a subset of layers can achieve performance close to full-layer LoRA while substantially reducing fine-tuning cost and the number of adapter-augmented layers during inference, offering a more cost-performance-aware alternative to full-layer insertion.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As large language models (LLMs) continue to grow, the cost of full-parameter fine-tuning has made parameter-efficient fine-tuning (PEFT) the default strategy for downstream adaptation.</div>
</details>
</div>
<div class="card">
<div class="title">Enhancing Mathematical Problem Solving in LLMs through Execution-Driven Reasoning Augmentation</div>
<div class="meta-line">Authors: Aditya Basarkar, Benyamin Tabarsi, Tiffany Barnes, Dongkuan, Xu</div>
<div class="meta-line">Venue: ACL</div>
<div class="meta-line">First: 2026-02-03T19:13:31+00:00 · Latest: 2026-02-03T19:13:31+00:00</div>
<div class="meta-line">Comments: 9 pages, 7 figures, submitted to ACL ARR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03950v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03950v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential. Although recent advances in multi-agent LLM-based systems have enhanced their mathematical reasoning capabilities, they still lack a reliably revisable representation of the reasoning process. Existing agents either operate in rigid sequential pipelines that cannot correct earlier steps or rely on heuristic self-evaluation that can fail to identify and fix errors. In addition, programmatic context can distract language models and degrade accuracy. To address these gaps, we introduce Iteratively Improved Program Construction (IIPC), a reasoning method that iteratively refines programmatic reasoning chains and combines execution feedback with the native Chain-of-thought abilities of the base LLM to maintain high-level contextual focus. IIPC surpasses competing approaches in the majority of reasoning benchmarks on multiple base LLMs. All code and implementations are released as open source.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Mathematical problem solving is a fundamental benchmark for assessing the reasoning capabilities of artificial intelligence and a gateway to applications in education, science, and engineering where reliable symbolic reasoning is essential.</div>
</details>
</div>
<div class="card">
<div class="title">Measuring Agents in Production</div>
<div class="meta-line">Authors: Melissa Z. Pan, Negar Arabzadeh, Riccardo Cogo, Yuxuan Zhu, Alexander Xiong, Lakshya A Agrawal, Huanzhi Mao, Emma Shen, Sid Pallerla, Liana Patel, Shu Liu, Tianneng Shi, Xiaoyuan Liu, Jared Quincy Davis, Emmanuele Lacavalla, Alessandro Basile, Shuyi Yang, Paul Castro, Daniel Kang, Joseph E. Gonzalez, Koushik Sen, Dawn Song, Ion Stoica, Matei Zaharia, Marquita Ellis</div>
<div class="meta-line">First: 2025-12-02T16:45:10+00:00 · Latest: 2026-02-03T18:06:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04123v3">Abs</a> · <a href="https://arxiv.org/pdf/2512.04123v3">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents already operate in production across many industries, yet we lack an understanding of what technical methods make deployments successful. We present the first systematic study of Measuring Agents in Production, MAP, using first-hand data from agent developers. We conducted 20 case studies via in-depth interviews and surveyed 306 practitioners across 26 domains. We investigate why organizations build agents, how they build them, how they evaluate them, and their top development challenges. Our study finds that production agents are built using simple, controllable approaches: 68% execute at most 10 steps before human intervention, 70% rely on prompting off-the-shelf models instead of weight tuning, and 74% depend primarily on human evaluation. Reliability (consistent correct behavior over time) remains the top development challenge, which practitioners currently address through systems-level design. MAP documents the current state of production agents, providing the research community with visibility into deployment realities and under-explored research avenues.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLM-based agents already operate in production across many industries, yet we lack an understanding of what technical methods make deployments successful.</div>
</details>
</div>
<div class="card">
<div class="title">Ontology-to-tools compilation for executable semantic constraint enforcement in LLM agents</div>
<div class="meta-line">Authors: Xiaochi Zhou, Patrick Bulter, Changxuan Yang, Simon D. Rihm, Thitikarn Angkanaporn, Jethro Akroyd, Sebastian Mosbach, Markus Kraft</div>
<div class="meta-line">First: 2026-02-03T12:03:26+00:00 · Latest: 2026-02-03T12:03:26+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03439v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03439v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">We introduce ontology-to-tools compilation as a proof-of-principle mechanism for coupling large language models (LLMs) with formal domain knowledge. Within The World Avatar (TWA), ontological specifications are compiled into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation rather than through post-hoc validation. Extending TWA&#x27;s semantic agent composition framework, the Model Context Protocol (MCP) and associated agents are integral components of the knowledge graph ecosystem, enabling structured interaction between generative models, symbolic constraints, and external resources. An agent-based workflow translates ontologies into ontology-aware tools and iteratively applies them to extract, validate, and repair structured knowledge from unstructured scientific text. Using metal-organic polyhedra synthesis literature as an illustrative case, we show how executable ontological semantics can guide LLM behaviour and reduce manual schema and prompt engineering, establishing a general paradigm for embedding formal knowledge into generative systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向LLM代理可执行语义约束强制的本体到工具编译</div>
<div class="mono" style="margin-top:8px">我们提出本体到工具编译作为原理验证机制，将大语言模型（LLM）与形式化领域知识耦合。在The World Avatar（TWA）框架中，本体规范被编译为可执行工具接口，基于LLM的代理必须通过这些接口创建和修改知识图谱实例，在生成过程中强制实施语义约束，而非通过事后验证。通过扩展TWA的语义代理组合框架，模型上下文协议（MCP）及其关联代理成为知识图谱生态系统的核心组件，实现生成模型、符号约束与外部资源间的结构化交互。基于代理的工作流将本体转化为本体感知工具，并迭代应用于从非结构化科学文本中提取、验证和修复结构化知识。以金属有机多面体合成文献为案例，我们展示了可执行本体语义如何引导LLM行为，减少人工模式设计与提示工程，为形式化知识嵌入生成系统建立了通用范式。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need to integrate large language models (LLMs) with formal domain knowledge for reliable structured knowledge generation, this research introduces ontology-to-tools compilation within The World Avatar (TWA) framework. The method compiles ontological specifications into executable tool interfaces that LLM-based agents must use to create and modify knowledge graph instances, enforcing semantic constraints during generation via the Model Context Protocol (MCP) and associated agents. Experimental results using metal-organic polyhedra synthesis literature demonstrate that this approach effectively guides LLM behavior to extract, validate, and repair structured knowledge from unstructured text, reducing the need for manual schema and prompt engineering.</div>
<div class="mono" style="margin-top:8px">本研究旨在将大型语言模型（LLM）与形式化领域知识结合，以实现可靠的、结构化知识生成，为此引入了本体到工具的编译机制。该方法在The World Avatar（TWA）框架内，将本体规范编译成可执行的工具接口，强制基于LLM的智能体在创建和修改知识图谱实例时通过模型上下文协议（MCP）及相关智能体在生成过程中执行语义约束。利用金属有机多面体合成文献的案例实验表明，该方法能有效引导LLM行为，从非结构化文本中提取、验证和修复结构化知识，减少了对人工模式设计和提示工程的依赖。</div>
</details>
</div>
<div class="card">
<div class="title">PACE: Pretrained Audio Continual Learning</div>
<div class="meta-line">Authors: Chang Li, Kanglei Zhou, Liyuan Wang</div>
<div class="meta-line">Venue: ICLR 2026</div>
<div class="meta-line">First: 2026-02-03T10:28:35+00:00 · Latest: 2026-02-03T10:28:35+00:00</div>
<div class="meta-line">Comments: Accepted at ICLR 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03355v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03355v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Audio is a fundamental modality for analyzing speech, music, and environmental sounds. Although pretrained audio models have significantly advanced audio understanding, they remain fragile in real-world settings where data distributions shift over time. In this work, we present the first systematic benchmark for audio continual learning (CL) with pretrained models (PTMs), together with a comprehensive analysis of its unique challenges. Unlike in vision, where parameter-efficient fine-tuning (PEFT) has proven effective for CL, directly transferring such strategies to audio leads to poor performance. This stems from a fundamental property of audio backbones: they focus on low-level spectral details rather than structured semantics, causing severe upstream-downstream misalignment. Through extensive empirical study, we identify analytic classifiers with first-session adaptation (FSA) as a promising direction, but also reveal two major limitations: representation saturation in coarse-grained scenarios and representation drift in fine-grained scenarios. To address these challenges, we propose PACE, a novel method that enhances FSA via a regularized analytic classifier and enables multi-session adaptation through adaptive subspace-orthogonal PEFT for improved semantic alignment. In addition, we introduce spectrogram-based boundary-aware perturbations to mitigate representation overlap and improve stability. Experiments on six diverse audio CL benchmarks demonstrate that PACE substantially outperforms state-of-the-art baselines, marking an important step toward robust and scalable audio continual learning with PTMs.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Audio is a fundamental modality for analyzing speech, music, and environmental sounds.</div>
</details>
</div>
<div class="card">
<div class="title">The Necessity of a Unified Framework for LLM-Based Agent Evaluation</div>
<div class="meta-line">Authors: Pengyu Zhu, Li Sun, Philip S. Yu, Sen Su</div>
<div class="meta-line">First: 2026-02-03T08:18:37+00:00 · Latest: 2026-02-03T08:18:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03238v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03238v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">With the advent of Large Language Models (LLMs), general-purpose agents have seen fundamental advancements. However, evaluating these agents presents unique challenges that distinguish them from static QA benchmarks. We observe that current agent benchmarks are heavily confounded by extraneous factors, including system prompts, toolset configurations, and environmental dynamics. Existing evaluations often rely on fragmented, researcher-specific frameworks where the prompt engineering for reasoning and tool usage varies significantly, making it difficult to attribute performance gains to the model itself. Additionally, the lack of standardized environmental data leads to untraceable errors and non-reproducible results. This lack of standardization introduces substantial unfairness and opacity into the field. We propose that a unified evaluation framework is essential for the rigorous advancement of agent evaluation. To this end, we introduce a proposal aimed at standardizing agent evaluation.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>构建基于LLM的智能体评估统一框架的必要性</div>
<div class="mono" style="margin-top:8px">随着大语言模型（LLM）的出现，通用智能体取得了根本性突破。然而，评估这些智能体面临着区别于静态问答基准的独特挑战。我们发现当前智能体基准测试受到系统提示、工具集配置和环境动态等外部因素的严重干扰。现有评估常依赖零散的研究者特定框架，其推理与工具使用的提示工程差异显著，导致难以将性能提升归因于模型本身。此外，标准化环境数据的缺失造成错误难以追踪、结果不可复现。这种标准化缺失给该领域带来了严重的不公平性和不透明性。我们认为，建立统一评估框架对严格推进智能体评估至关重要。为此，我们提出了一项旨在标准化智能体评估的方案。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the observation that current LLM-based agent evaluations are confounded by inconsistent variables like system prompts, tool configurations, and environmental dynamics—leading to unfair comparisons and irreproducible results—this work argues for a unified evaluation framework. The method proposes standardizing agent assessment to isolate model capabilities from extraneous factors. Key findings highlight that fragmented existing frameworks obscure performance attribution and introduce opacity, necessitating standardization for rigorous progress in agent evaluation.</div>
<div class="mono" style="margin-top:8px">本文针对当前基于大语言模型的智能体评估中存在的混杂因素（如系统提示、工具配置和环境动态不一致）导致不公平比较和结果不可复现的问题，提出建立统一评估框架的必要性。方法上主张通过标准化评估流程分离模型能力与外部变量。核心发现指出，现有碎片化框架使性能归因模糊并缺乏透明度，需标准化以推动智能体评估的严谨发展。</div>
</details>
</div>
<div class="card">
<div class="title">Understanding Multi-Agent LLM Frameworks: A Unified Benchmark and Experimental Analysis</div>
<div class="meta-line">Authors: Abdelghny Orogat, Ana Rostam, Essam Mansour</div>
<div class="meta-line">First: 2026-02-03T05:37:56+00:00 · Latest: 2026-02-03T05:37:56+00:00</div>
<div class="meta-line">Comments: 25 pages, 9 figures and 13 tables; introduces MAFBench unified multi-agent evaluation suite</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03128v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03128v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs). These frameworks impose distinct architectural structures that govern how agents interact, store information, and coordinate tasks. However, their impact on system performance remains poorly understood. This gap is critical, as architectural choices alone can induce order-of-magnitude differences in latency and throughput, as well as substantial variation in accuracy and scalability. Addressing this challenge requires (i) jointly evaluating multiple capabilities, such as orchestration overhead, memory behavior, planning, specialization, and coordination, and (ii) conducting these evaluations under controlled, framework-level conditions to isolate architectural effects. Existing benchmarks focus on individual capabilities and lack standardized framework-level evaluation. We address these limitations by (i) introducing an architectural taxonomy for systematically comparing multi-agent LLM frameworks along fundamental dimensions, and (ii) developing MAFBench, a unified evaluation suite that integrates existing benchmarks under a standardized execution pipeline. Using MAFBench, we conduct a controlled empirical study across several widely used frameworks. Our results show that framework-level design choices alone can increase latency by over 100x, reduce planning accuracy by up to 30%, and lower coordination success from above 90% to below 30%. Finally, we translate our findings into concrete architectural design principles and framework selection guidance, and outline promising future research directions.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-agent LLM frameworks are widely used to accelerate the development of agent systems powered by large language models (LLMs).</div>
</details>
</div>
<div class="card">
<div class="title">Multi-Agent Teams Hold Experts Back</div>
<div class="meta-line">Authors: Aneesh Pappu, Batu El, Hancheng Cao, Carmelo di Nolfo, Yanchao Sun, Meng Cao, James Zou</div>
<div class="meta-line">First: 2026-02-01T04:34:36+00:00 · Latest: 2026-02-03T04:46:07+00:00</div>
<div class="meta-line">Comments: Preprint</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01011v2">Abs</a> · <a href="https://arxiv.org/pdf/2602.01011v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows. In such settings, effective coordination cannot be fully designed in advance and must instead emerge through interaction. However, most prior work enforces coordination through fixed roles, workflows, or aggregation rules, leaving open the question of how well self-organizing teams perform when coordination is unconstrained. Drawing on organizational psychology, we study whether self-organizing LLM teams achieve strong synergy, where team performance matches or exceeds the best individual member. Across human-inspired and frontier ML benchmarks, we find that -- unlike human teams -- LLM teams consistently fail to match their expert agent&#x27;s performance, even when explicitly told who the expert is, incurring performance losses of up to 37.6%. Decomposing this failure, we show that expert leveraging, rather than identification, is the primary bottleneck. Conversational analysis reveals a tendency toward integrative compromise -- averaging expert and non-expert views rather than appropriately weighting expertise -- which increases with team size and correlates negatively with performance. Interestingly, this consensus-seeking behavior improves robustness to adversarial agents, suggesting a trade-off between alignment and effective expertise utilization. Our findings reveal a significant gap in the ability of self-organizing multi-agent teams to harness the collective expertise of their members.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Multi-agent LLM systems are increasingly deployed as autonomous collaborators, where agents interact freely rather than execute fixed, pre-specified workflows.</div>
</details>
</div>
<div class="card">
<div class="title">FedKRSO: Communication and Memory Efficient Federated Fine-Tuning of Large Language Models</div>
<div class="meta-line">Authors: Guohao Yang, Tongle Wu, Yuanxiong Guo, Ying Sun, Yanmin Gong</div>
<div class="meta-line">First: 2026-02-03T02:39:33+00:00 · Latest: 2026-02-03T02:39:33+00:00</div>
<div class="meta-line">Comments: Accepted by INFOCOM 2026</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.03019v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.03019v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Fine-tuning is essential to adapt general-purpose large language models (LLMs) to domain-specific tasks. As a privacy-preserving framework to leverage decentralized data for collaborative model training, Federated Learning (FL) is gaining popularity in LLM fine-tuning, but remains challenging due to the high cost of transmitting full model parameters and computing full gradients on resource-constrained clients. While Parameter-Efficient Fine-Tuning (PEFT) methods are widely used in FL to reduce communication and memory costs, they often sacrifice model performance compared to FFT. This paper proposes FedKRSO (Federated $K$-Seed Random Subspace Optimization), a novel method that enables communication and memory efficient FFT of LLMs in federated settings. In FedKRSO, clients update the model within a shared set of random low-dimension subspaces generated by the server to save memory usage. Furthermore, instead of transmitting full model parameters in each FL round, clients send only the model update accumulators along the subspaces to the server, enabling efficient global model aggregation and dissemination. By using these strategies, FedKRSO can substantially reduce communication and memory overhead while overcoming the performance limitations of PEFT, closely approximating the performance of federated FFT. The convergence properties of FedKRSO are analyzed rigorously under general FL settings. Extensive experiments on the GLUE benchmark across diverse FL scenarios demonstrate that FedKRSO achieves both superior performance and low communication and memory overhead, paving the way towards on federated LLM fine-tuning at the resource-constrained edge.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the high communication and memory costs of federated full fine-tuning (FFT) for large language models and the performance limitations of parameter-efficient fine-tuning (PEFT) methods in federated learning, this paper proposes FedKRSO. The method enables efficient FFT by having clients update models within server-generated random low-dimensional subspaces to save memory, and transmitting only subspace accumulators instead of full parameters for communication-efficient global aggregation. Experimental results on the GLUE benchmark across diverse federated scenarios demonstrate that FedKRSO achieves performance close to federated FFT while significantly reducing communication and memory overhead, overcoming PEFT&#x27;s performance limitations, with its convergence rigorously analyzed.</div>
<div class="mono" style="margin-top:8px">针对大型语言模型在联邦学习中全参数微调（FFT）通信与内存成本高昂，以及参数高效微调（PEFT）方法性能受限的问题，本文提出FedKRSO方法。该方法通过客户端在服务器生成的随机低维子空间内更新模型以节省内存，并仅传输子空间累加器而非完整参数，实现高效的全局聚合。在GLUE基准测试上的多场景联邦实验表明，FedKRSO在显著降低通信与内存开销的同时，性能逼近联邦FFT，克服了PEFT的性能局限，其收敛性在通用联邦设定下得到严格分析。</div>
</details>
</div>
<div class="card">
<div class="title">Natural Language Actor-Critic: Scalable Off-Policy Learning in Language Space</div>
<div class="meta-line">Authors: Joey Hong, Kang Liu, Zhan Ling, Jiecao Chen, Sergey Levine</div>
<div class="meta-line">First: 2025-12-04T09:21:44+00:00 · Latest: 2026-02-02T22:54:14+00:00</div>
<div class="meta-line">Comments: 21 pages, 4 figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2512.04601v2">Abs</a> · <a href="https://arxiv.org/pdf/2512.04601v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM) agents -- LLMs that dynamically interact with an environment over long horizons -- have become an increasingly important area of research, enabling automation in complex tasks involving tool-use, web browsing, and dialogue with people. In the absence of expert demonstrations, training LLM agents has relied on policy gradient methods that optimize LLM policies with respect to an (often sparse) reward function. However, in long-horizon tasks with sparse rewards, learning from trajectory-level rewards can be noisy, leading to training that is unstable and has high sample complexity. Furthermore, policy improvement hinges on discovering better actions through exploration, which can be difficult when actions lie in natural language space. In this paper, we propose Natural Language Actor-Critic (NLAC), a novel actor-critic algorithm that trains LLM policies using a generative LLM critic that produces natural language rather than scalar values. This approach leverages the inherent strengths of LLMs to provide a richer and more actionable training signal; particularly, in tasks with large, open-ended action spaces, natural language explanations for why an action is suboptimal can be immensely useful for LLM policies to reason how to improve their actions, without relying on random exploration. Furthermore, our approach can be trained off-policy without policy gradients, offering a more data-efficient and stable alternative to existing on-policy methods. We present results on a mixture of reasoning, web browsing, and tool-use with dialogue tasks, demonstrating that NLAC shows promise in outperforming existing training approaches and offers a more scalable and stable training paradigm for LLM agents.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Training large language model (LLM) agents for long-horizon tasks with sparse rewards faces challenges including noisy trajectory-level rewards, high sample complexity, and inefficient exploration in natural language action spaces. To address this, the authors propose Natural Language Actor-Critic (NLAC), an off-policy actor-critic algorithm that replaces scalar critics with a generative LLM critic producing natural language explanations of suboptimal actions, enabling richer feedback without policy gradients. Experiments across reasoning, web browsing, and tool-use dialogue tasks demonstrate NLAC outperforms existing methods, offering more stable, scalable, and data-efficient training.</div>
<div class="mono" style="margin-top:8px">针对稀疏奖励的长时程任务中大型语言模型（LLM）智能体训练存在的轨迹级奖励噪声大、样本复杂度高及自然语言动作空间探索低效等问题，本文提出自然语言演员-评论家（NLAC）方法。该方法采用离策略演员-评论家框架，用生成式LLM评论家替代标量评论家，生成解释动作缺陷的自然语言反馈，无需策略梯度即可提供更丰富的改进信号。在推理、网页浏览和工具使用对话任务的实验中，NLAC展现出优于现有方法的性能，提供了更稳定、可扩展且数据高效的训练范式。</div>
</details>
</div>
<div class="card">
<div class="title">AutoSizer: Automatic Sizing of Analog and Mixed-Signal Circuits via Large Language Model (LLM) Agents</div>
<div class="meta-line">Authors: Xi Yu, Dmitrii Torbunov, Soumyajit Mandal, Yihui Ren</div>
<div class="meta-line">First: 2026-02-02T21:51:55+00:00 · Latest: 2026-02-02T21:51:55+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02849v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02849v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints. Existing Electronic Design Automation (EDA) methods typically frame sizing as static black-box optimization, resulting in inefficient and less robust solutions. Although Large Language Models (LLMs) exhibit strong reasoning abilities, they are not suited for precise numerical optimization in AMS sizing. To address this gap, we propose AutoSizer, a reflective LLM-driven meta-optimization framework that unifies circuit understanding, adaptive search-space construction, and optimization orchestration in a closed loop. It employs a two-loop optimization framework, with an inner loop for circuit sizing and an outer loop that analyzes optimization dynamics and constraints to iteratively refine the search space from simulation feedback. We further introduce AMS-SizingBench, an open benchmark comprising 24 diverse AMS circuits in SKY130 CMOS technology, designed to evaluate adaptive optimization policies under realistic simulator-based constraints. AutoSizer experimentally achieves higher solution quality, faster convergence, and higher success rate across varying circuit difficulties, outperforming both traditional optimization methods and existing LLM-based agents.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>AutoSizer：基于大语言模型（LLM）代理的模拟与混合信号电路自动尺寸调整</div>
<div class="mono" style="margin-top:8px">模拟与混合信号（AMS）集成电路设计仍严重依赖专家知识，其中晶体管尺寸调整因非线性行为、高维设计空间及严格性能约束成为主要瓶颈。现有电子设计自动化（EDA）方法通常将尺寸调整视为静态黑箱优化，导致解决方案效率低下且鲁棒性不足。尽管大语言模型（LLM）展现出强大推理能力，却不适于AMS尺寸调整中的精确数值优化。为此，我们提出AutoSizer——一种反射式LLM驱动的元优化框架，通过闭环系统将电路理解、自适应搜索空间构建与优化编排相统一。该框架采用双循环优化结构：内循环执行电路尺寸调整，外循环分析优化动态与约束条件，并基于仿真反馈迭代优化搜索空间。我们进一步推出AMS-SizingBench开放基准测试集，包含采用SKY130 CMOS工艺的24个多样化AMS电路，用于评估真实仿真器约束下的自适应优化策略。实验表明，AutoSizer在不同电路复杂度下均实现更优解质量、更快收敛速度及更高成功率，性能超越传统优化方法与现有LLM代理。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The design of Analog and Mixed-Signal (AMS) integrated circuits remains heavily reliant on expert knowledge, with transistor sizing a major bottleneck due to nonlinear behavior, high-dimensional design spaces, and strict performance constraints.</div>
</details>
</div>
<div class="card">
<div class="title">From Task Solving to Robust Real-World Adaptation in LLM Agents</div>
<div class="meta-line">Authors: Pouya Pezeshkpour, Estevam Hruschka</div>
<div class="meta-line">First: 2026-02-02T20:10:40+00:00 · Latest: 2026-02-02T20:10:40+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02760v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02760v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons. Yet many existing evaluations assume a &quot;clean interface&quot; where dynamics are specified and stable, tools and sensors are reliable, and success is captured by a single explicit objective-often overestimating real-world readiness. In practice, agents face underspecified rules, unreliable signals, shifting environments, and implicit, multi-stakeholder goals. The challenge is therefore not just solving tasks, but adapting while solving: deciding what to trust, what is wanted, when to verify, and when to fall back or escalate. We stress-test deployment-relevant robustness under four operational circumstances: partial observability, dynamic environments, noisy signals, and dynamic agent state. We benchmark agentic LLMs in a grid-based game with a simple goal but long-horizon execution. Episodes violate clean-interface assumptions yet remain solvable, forcing agents to infer rules, pay for information, adapt to environmental and internal shifts, and act cautiously under noise. Across five state-of-the-art LLM agents, we find large gaps between nominal task-solving and deployment-like robustness. Performance generally degrades as grid size and horizon increase, but rankings are unstable: weaker models can beat stronger ones when strategy matches the uncertainty regime. Despite no explicit instruction, agents trade off completion, efficiency, and penalty avoidance, suggesting partial objective inference. Ablations and feature analyses reveal model-specific sensitivities and failure drivers, motivating work on verification, safe action selection, and objective inference under partial observability, noise, and non-stationarity.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Large language models are increasingly deployed as specialized agents that plan, call tools, and take actions over extended horizons.</div>
</details>
</div>
<div class="card">
<div class="title">MARS: Modular Agent with Reflective Search for Automated AI Research</div>
<div class="meta-line">Authors: Jiefeng Chen, Bhavana Dalvi Mishra, Jaehyun Nam, Rui Meng, Tomas Pfister, Jinsung Yoon</div>
<div class="meta-line">First: 2026-02-02T19:00:03+00:00 · Latest: 2026-02-02T19:00:03+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02660v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02660v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution. Current LLM-based agents struggle here, often generating monolithic scripts that ignore execution costs and causal factors. We introduce MARS (Modular Agent with Reflective Search), a framework optimized for autonomous AI research. MARS relies on three pillars: (1) Budget-Aware Planning via cost-constrained Monte Carlo Tree Search (MCTS) to explicitly balance performance with execution expense; (2) Modular Construction, employing a &quot;Design-Decompose-Implement&quot; pipeline to manage complex research repositories; and (3) Comparative Reflective Memory, which addresses credit assignment by analyzing solution differences to distill high-signal insights. MARS achieves state-of-the-art performance among open-source frameworks on MLE-Bench under comparable settings, maintaining competitiveness with the global leaderboard&#x27;s top methods. Furthermore, the system exhibits qualitative &quot;Aha!&quot; moments, where 63% of all utilized lessons originate from cross-branch transfer, demonstrating that the agent effectively generalizes insights across search paths.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Automating AI research differs from general software engineering due to computationally expensive evaluation (e.g., model training) and opaque performance attribution.</div>
</details>
</div>
<div class="card">
<div class="title">Fat-Cat: Document-Driven Metacognitive Multi-Agent System for Complex Reasoning</div>
<div class="meta-line">Authors: Tong Yang, Yemin Wang, Chaoning Zhang, Aming Wu</div>
<div class="meta-line">First: 2026-02-02T15:12:13+00:00 · Latest: 2026-02-02T15:12:13+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02206v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02206v1">PDF</a> · <a href="https://github.com/answeryt/Fat-Cat">Code1</a> · <a href="https://huggingface.co/huggingface">Code2</a> · <a href="https://huggingface.co/docs/hub/spaces">Code3</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime. Existing agent frameworks rely on rigid, syntax-heavy state representations such as nested JSON, which require models to devote a substantial portion of their limited attention to syntactic processing rather than semantic reasoning. In this paper, we propose Fat-Cat, a document-driven agent architecture that improves the signal-to-noise ratio of state management. By integrating three key components: (1) a Semantic File System that represents agent state as Markdown documents aligned with common pre-training corpora, (2) a Textual Strategy Evolution module that accumulates task-solving knowledge without parameter updates, and (3) a Closed-Loop Watcher that monitors reasoning trajectories to reduce hallucinations. Extensive reasoning, retrieval, and coding benchmarks, Fat-Cat consistently improves agent performance. It enables the Kimi-k2 model to outperform the proprietary GPT-4o baseline on HotPotQA. Replacing the document-based state with JSON leads to performance drop, while empirically validating the critical necessity of document-driven state modeling over rigid syntax. The code is available at https://github.com/answeryt/Fat-Cat.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The effectiveness of LLM-based agents is often limited not by model capacity alone, but by how efficiently contextual information is utilized at runtime.</div>
</details>
</div>
<div class="card">
<div class="title">Constrained Process Maps for Multi-Agent Generative AI Workflows</div>
<div class="meta-line">Authors: Ananya Joshi, Michael Rudow</div>
<div class="meta-line">First: 2026-02-02T12:32:11+00:00 · Latest: 2026-02-02T12:32:11+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.02034v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.02034v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Large language model (LLM)-based agents are increasingly used to perform complex, multi-step workflows in regulated settings such as compliance and due diligence. However, many agentic architectures rely primarily on prompt engineering of a single agent, making it difficult to observe or compare how models handle uncertainty and coordination across interconnected decision stages and with human oversight. We introduce a multi-agent system formalized as a finite-horizon Markov Decision Process (MDP) with a directed acyclic structure. Each agent corresponds to a specific role or decision stage (e.g., content, business, or legal review in a compliance workflow), with predefined transitions representing task escalation or completion. Epistemic uncertainty is quantified at the agent level using Monte Carlo estimation, while system-level uncertainty is captured by the MDP&#x27;s termination in either an automated labeled state or a human-review state. We illustrate the approach through a case study in AI safety evaluation for self-harm detection, implemented as a multi-agent compliance system. Results demonstrate improvements over a single-agent baseline, including up to a 19\% increase in accuracy, up to an 85x reduction in required human review, and, in some configurations, reduced processing time.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Motivated by the need for transparent and auditable multi-step AI workflows in regulated domains like compliance, this research addresses limitations of single-agent LLM systems in handling uncertainty and coordination. The authors propose a multi-agent framework formalized as a finite-horizon Markov Decision Process with directed acyclic structure, where specialized agents represent distinct workflow stages (e.g., content or legal review) and quantify epistemic uncertainty via Monte Carlo estimation. In a case study on AI safety evaluation for self-harm detection, the method demonstrated up to 19% higher accuracy, 85x reduced human review burden, and faster processing compared to single-agent baselines.</div>
<div class="mono" style="margin-top:8px">针对合规等受监管领域中多步骤AI工作流对透明性与可审计性的需求，本研究解决了单一智能体LLM系统在处理跨阶段不确定性与协作时的不足。作者提出一种形式化为有限时域马尔可夫决策过程的多智能体框架，采用有向无环结构：专用智能体代表不同工作流阶段（如内容或法律审核），通过蒙特卡洛估计量化认知不确定性。在自残检测AI安全评估案例中，该方法相较单智能体基线实现了最高19%的准确率提升、85倍人工审核量减少及更快的处理速度。</div>
</details>
</div>
<div class="card">
<div class="title">ORCH: many analyses, one merge-a deterministic multi-agent orchestrator for discrete-choice reasoning with EMA-guided routing</div>
<div class="meta-line">Authors: Hanlin Zhou, Huah Yong Chan</div>
<div class="meta-line">First: 2026-02-02T08:27:58+00:00 · Latest: 2026-02-02T08:27:58+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01797v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01797v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks. However, many existing systems rely on stochastic routing or ad-hoc heuristics, making their behavior difficult to reproduce and their decision process hard to interpret. We propose ORCH, a deterministic coordination framework for discrete-choice reasoning that orchestrates heterogeneous LLMs. ORCH follows a ``many analyses, one decision&#x27;&#x27; paradigm: multiple base models independently produce structured analyses, and a dedicated merge agent outputs the final choice. The framework uses fixed rules for task decomposition and answer aggregation, keeping the pipeline predictable, reproducible, and training-free. Determinism here refers to fixed routing and aggregation rules under a fixed evaluation protocol, rather than strict bit-level reproducibility across deployments. To exploit model complementarity, we optionally introduce an EMA-guided router that updates agent selection using historical accuracy, latency, or cost; since it relies on answer-based feedback, it is mainly intended for benchmarking, controlled evaluation, or delayed-feedback settings. Experiments on MMLU, MMLU-Pro, and GSM8K show that ORCH consistently outperforms single-model baselines and a majority-vote ensemble. On MMLU-Pro, ORCH improves accuracy by over 10 points compared to the strongest baseline, and on GSM8K it yields gains exceeding 50 points; McNemar tests confirm statistical significance. The EMA router provides an additional 0.7--2.0 point accuracy boost, and ablations show that both multi-agent collaboration and routing contribute substantially. Overall, ORCH offers a practical path toward controllable, interpretable, and deployment-ready LLM-based agent systems for discrete-choice reasoning.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Recent advances in large-scale language models (LLMs) have made multi-agent architectures attractive for challenging reasoning tasks.</div>
</details>
</div>
<div class="card">
<div class="title">TRIP-Bench: A Benchmark for Long-Horizon Interactive Agents in Real-World Scenarios</div>
<div class="meta-line">Authors: Yuanzhe Shen, Zisu Huang, Zhengyuan Wang, Muzhao Tian, Zhengkang Guo, Chenyang Zhang, Shuaiyu Zhou, Zengjie Hu, Dailin Li, Jingwen Xu, Kaimin Wang, Wenhao Liu, Tianlong Li, Fengpeng Yue, Feng Hong, Cao Liu, Ke Zeng</div>
<div class="meta-line">First: 2026-02-02T05:43:08+00:00 · Latest: 2026-02-02T05:43:08+00:00</div>
<div class="meta-line">Comments: 40 pages, 6figures</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01675v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01675v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions. To bridge this gap, we introduce \textbf{TRIP-Bench}, a long-horizon benchmark grounded in realistic travel-planning scenarios. TRIP-Bench leverages real-world data, offers 18 curated tools and 40+ travel requirements, and supports automated evaluation. It includes splits of varying difficulty; the hard split emphasizes long and ambiguous interactions, style shifts, feasibility changes, and iterative version revision. Dialogues span up to 15 user turns, can involve 150+ tool calls, and may exceed 200k tokens of context. Experiments show that even advanced models achieve at most 50\% success on the easy split, with performance dropping below 10\% on hard subsets. We further propose \textbf{GTPO}, an online multi-turn reinforcement learning method with specialized reward normalization and reward differencing. Applied to Qwen2.5-32B-Instruct, GTPO improves constraint satisfaction and interaction robustness, outperforming Gemini-3-Pro in our evaluation. We expect TRIP-Bench to advance practical long-horizon interactive agents, and GTPO to provide an effective online RL recipe for robust long-horizon training.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">As LLM-based agents are deployed in increasingly complex real-world settings, existing benchmarks underrepresent key challenges such as enforcing global constraints, coordinating multi-tool reasoning, and adapting to evolving user behavior over long, multi-turn interactions.</div>
</details>
</div>
<div class="card">
<div class="title">JudgeFlow: Agentic Workflow Optimization via Block Judge</div>
<div class="meta-line">Authors: Zihan Ma, Zhikai Zhao, Chuanbo Hua, Federico Berto, Jinkyoo Park</div>
<div class="meta-line">First: 2026-01-12T12:30:14+00:00 · Latest: 2026-02-02T04:22:18+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.07477v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.07477v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities. Current methods rely on coarse, end-to-end evaluation signals and lack fine-grained signals on where to refine, often resulting in inefficient or low-impact modifications. To address these limitations, we propose JudgeFlow, an Evaluation-Judge-Optimization-Update pipeline. We incorporate reusable, configurable logic blocks into agentic workflows to capture fundamental forms of logic. On top of this abstraction, we design a dedicated Judge module that inspects execution traces particularly failed runs and assigns rank-based responsibility scores to problematic blocks. These fine-grained diagnostic signals are then leveraged by an LLM-based optimizer, which focuses modifications on the most problematic block in the workflow. Our approach improves sample efficiency, enhances interpretability through block-level diagnostics, and provides a scalable foundation for automating increasingly complex agentic workflows. We evaluate JudgeFlow on mathematical reasoning and code generation benchmarks, where JudgeFlow achieves superior performance and efficiency compared to existing methods.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Optimizing LLM-based agentic workflows is challenging for scaling AI capabilities.</div>
</details>
</div>
<div class="card">
<div class="title">TransportAgents: a multi-agents LLM framework for traffic accident severity prediction</div>
<div class="meta-line">Authors: Zhichao Yang, Jiashu He, Jinxuan Fan, Cirillo Cinzia</div>
<div class="meta-line">First: 2026-01-21T23:14:05+00:00 · Latest: 2026-02-02T04:18:37+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2601.15519v2">Abs</a> · <a href="https://arxiv.org/pdf/2601.15519v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning. Although recent large language models (LLMs) exhibit strong reasoning capabilities, their single-agent architectures often struggle with heterogeneous, domain-specific crash data and tend to generate biased or unstable predictions. To address these limitations, this paper proposes TransportAgents, a hybrid multi-agent framework that integrates category-specific LLM reasoning with a multilayer perceptron (MLP) integration module. Each specialized agent focuses on a particular subset of traffic information, such as demographics, environmental context, or incident details, to produce intermediate severity assessments that are subsequently fused into a unified prediction. Extensive experiments on two complementary U.S. datasets, the Consumer Product Safety Risk Management System (CPSRMS) and the National Electronic Injury Surveillance System (NEISS), demonstrate that TransportAgents consistently outperforms both traditional machine learning and advanced LLM-based baselines. Across three representative backbones, including closed-source models such as GPT-3.5 and GPT-4o, as well as open-source models such as LLaMA-3.3, the framework exhibits strong robustness, scalability, and cross-dataset generalizability. A supplementary distributional analysis further shows that TransportAgents produces more balanced and well-calibrated severity predictions than standard single-agent LLM approaches, highlighting its interpretability and reliability for safety-critical decision support applications.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Accurate prediction of traffic crash severity is critical for improving emergency response and public safety planning.</div>
</details>
</div>
<div class="card">
<div class="title">When Is Rank-1 Enough? Geometry-Guided Initialization for Parameter-Efficient Fine-Tuning</div>
<div class="meta-line">Authors: Haoran Zhao, Soyeon Caren Han, Eduard Hovy</div>
<div class="meta-line">First: 2026-02-02T01:31:25+00:00 · Latest: 2026-02-02T01:31:25+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01522v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01522v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable. We show that this instability is not solely due to limited capacity: in the rank-1 regime, optimization is highly sensitive to the update direction. Concretely, pretrained vision and text features form mismatched anisotropic regions, yielding a dominant &quot;gap&quot; direction that acts like a translation component and disproportionately steers early gradients under rank-1 constraints. Analyzing pretrained representations, we identify a modality-gap axis that dominates early gradient flow, while a random rank-1 initialization is unlikely to align with it, leading to weak gradients and training collapse. We propose Gap-Init, a geometry-aware initialization that aligns the rank-1 LoRA direction with an estimated modality-gap vector from a small calibration set, while keeping the initial LoRA update zero. Across multiple vision-language tasks and backbones, Gap-Init consistently stabilizes rank-1 training and can match or outperform strong rank-8 baselines. Our results suggest that at the extreme low-rank limit, initial alignment can matter as much as rank itself.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Parameter-efficient fine-tuning (PEFT) is a standard way to adapt multimodal large language models, yet extremely low-rank settings -- especially rank-1 LoRA -- are often unstable.</div>
</details>
</div>
<div class="card">
<div class="title">Workflow-R1: Group Sub-sequence Policy Optimization for Multi-turn Workflow Construction</div>
<div class="meta-line">Authors: Mingze Kong, Zikun Qu, Zhongquan Zhou, Pengyu Liang, Xiang Li, Zhiwei Shang, Zhi Hong, Kaiyu Huang, Zhiyong Wang, Zhongxiang Dai</div>
<div class="meta-line">First: 2026-02-01T12:44:59+00:00 · Latest: 2026-02-01T12:44:59+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01202v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01202v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks. However, existing workflow optimization methods typically formulate workflow synthesis as a static, one-shot code-centric generation problem. This paradigm imposes excessive constraints on the model&#x27;s coding capabilities and restricts the flexibility required for dynamic problem-solving. In this paper, we present Workflow-R1, a framework that reformulates workflow construction as a multi-turn, natural language-based sequential decision-making process. To resolve the optimization granularity mismatch inherent in such multi-turn interactions, we introduce Group Sub-sequence Policy Optimization (GSsPO). While explicitly tailored to align with the interleaved Think-Action dynamics of agentic reasoning, GSsPO fundamentally functions as a structure-aware RL algorithm generalizable to a broad class of multi-turn agentic sequential decision-making tasks. By recalibrating the optimization unit to the composite sub-sequence, specifically the atomic Think-Action cycle, it aligns gradient updates with the semantic boundaries of these interactions, ensuring robust learning in complex multi-turn reasoning tasks. Through extensive experiments on multiple QA benchmarks, Workflow-R1 outperforms competitive baselines, validating GSsPO as a generalized solution for sequential reasoning and establishing Workflow-R1 as a promising new paradigm for automated workflow optimization.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">The rapid evolution of agentic workflows has demonstrated strong performance of LLM-based agents in addressing complex reasoning tasks.</div>
</details>
</div>
<div class="card">
<div class="title">AutoHealth: An Uncertainty-Aware Multi-Agent System for Autonomous Health Data Modeling</div>
<div class="meta-line">Authors: Tong Xia, Weibin Li, Gang Liu, Yong Li</div>
<div class="meta-line">First: 2026-02-01T07:50:15+00:00 · Latest: 2026-02-01T07:50:15+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.01078v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.01078v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited. Existing systems often struggle to generalize across heterogeneous health data modalities, rely heavily on predefined solution templates with insufficient adaptation to task-specific objectives, and largely overlook uncertainty estimation, which is essential for reliable decision-making in healthcare. To address these challenges, we propose \textit{AutoHealth}, a novel uncertainty-aware multi-agent system that autonomously models health data and assesses model reliability. \textit{AutoHealth} employs closed-loop coordination among five specialized agents to perform data exploration, task-conditioned model construction, training, and optimization, while jointly prioritizing predictive performance and uncertainty quantification. Beyond producing ready-to-use models, the system generates comprehensive reports to support trustworthy interpretation and risk-aware decision-making. To rigorously evaluate its effectiveness, we curate a challenging real-world benchmark comprising 17 tasks across diverse data modalities and learning settings. \textit{AutoHealth} completes all tasks and outperforms state-of-the-art baselines by 29.2\% in prediction performance and 50.2\% in uncertainty estimation.</div></details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">LLM-based agents have demonstrated strong potential for autonomous machine learning, yet their applicability to health data remains limited.</div>
</details>
</div>
<div class="card">
<div class="title">Hypernetwork-Driven Low-Rank Adaptation Across Attention Heads</div>
<div class="meta-line">Authors: Nghiem T. Diep, Dung Le, Tuan Truong, Tan Dinh, Huy Nguyen, Nhat Ho</div>
<div class="meta-line">First: 2025-10-05T17:13:39+00:00 · Latest: 2026-01-31T22:34:25+00:00</div>
<div class="meta-line">Comments: Nghiem T. Diep, Dung Le, and Tuan Truong contributed equally to this work</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2510.04295v2">Abs</a> · <a href="https://arxiv.org/pdf/2510.04295v2">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Parameter-efficient fine-tuning (PEFT) has emerged as a powerful paradigm for adapting large-scale pre-trained models to downstream tasks with minimal additional parameters. Among PEFT methods, Low-Rank Adaptation (LoRA) stands out for its effectiveness by inserting trainable low-rank matrices into weight updates to enable efficient adaptation. However, when applied to multi-head self-attention, existing LoRA-based methods typically fine-tune each attention head independently, overlooking potential interactions and shared structure among heads. To address this limitation, we propose Hypernetwork-Driven Low-rank Adaptation (HyRA) that employs a hypernetwork to generate joint low-rank matrices for all attention heads within a layer. The shared generator promotes cross-head information sharing, helping low-rank modules avoid the redundant feature learning seen in traditional LoRA methods. Theoretically, our method achieves significantly better sample efficiency compared to standard LoRA. Empirically, we evaluate HyRA on a comprehensive suite of language and vision benchmarks. Our approach consistently outperforms existing parameter-efficient fine-tuning (PEFT) baselines across a wide range of tasks. Notably, in low-data regimes, HyRA achieves substantial improvements over LoRA, underscoring its practical sample efficiency and effectiveness in data-scarce scenarios.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>超网络驱动的跨注意力头低秩适配</div>
<div class="mono" style="margin-top:8px">参数高效微调（PEFT）已成为将大规模预训练模型适配至下游任务的重要范式，其仅需引入极少额外参数。在各类PEFT方法中，低秩适配（LoRA）通过向权重更新注入可训练低秩矩阵实现高效适配，成效显著。然而应用于多头自注意力机制时，现有基于LoRA的方法通常独立微调各注意力头，忽视了头间潜在交互与共享结构。为此，我们提出超网络驱动的低秩适配（HyRA），利用超网络为层内所有注意力头生成联合低秩矩阵。共享生成器促进跨头信息交互，使低秩模块避免传统LoRA方法中的冗余特征学习。理论证明本方法较标准LoRA具有显著更优的样本效率。实证层面，我们在全面语言与视觉基准测试中评估HyRA。该方法在广泛任务中持续超越现有参数高效微调（PEFT）基线，尤其在低数据量场景下较LoRA实现显著提升，彰显其在数据稀缺情境中的实用样本效率与有效性。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Parameter-efficient fine-tuning (PEFT) has emerged as a powerful paradigm for adapting large-scale pre-trained models to downstream tasks with minimal additional parameters.</div>
</details>
</div>
<div class="card">
<div class="title">Synapse Compendium Aware Federated Knowledge Exchange for Tool Routed LLMs</div>
<div class="meta-line">Authors: Abhijit Chakraborty, Sandipan De, Yash Shah, Chahana Dahal, Vivek Gupta</div>
<div class="meta-line">First: 2026-01-31T21:43:48+00:00 · Latest: 2026-01-31T21:43:48+00:00</div>
<div class="links" style="margin-top:8px"><a href="https://arxiv.org/abs/2602.00911v1">Abs</a> · <a href="https://arxiv.org/pdf/2602.00911v1">PDF</a> · <a href="https://huggingface.co/huggingface">Code1</a> · <a href="https://huggingface.co/docs/hub/spaces">Code2</a></div>
<details class="detail"><summary>Abstract</summary>
<div class="mono">Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness. We introduce Synapse, a framework that trains a shared global knowledge model of tool-usage behavior. Client agents with fixed LLMs learn tool-usage patterns locally, and transmit artifacts for federated aggregation through coordinators. A global tool compendium is updated and redistributed, enabling convergence toward stable tool selection. Synapse uses templated representations, embedding retrieval with LLM reranking, and adaptive masking to maintain utility while limiting information leakage. The framework supports heterogeneous data and quantifies performance improvements. Results show that Synapse improves tool-usage effectiveness and reduces communication overhead compared with weight or prompt-sharing approaches in multi-agent LLM systems.</div></details>
<details class="detail"><summary>中文标题/摘要</summary>
<div class="mono"><b>标题：</b>面向工具路由大语言模型的突触知识库感知联邦知识交换框架</div>
<div class="mono" style="margin-top:8px">联邦学习环境下基于大语言模型（LLM）的智能体协同学习面临通信成本、数据异构性及工具使用差异等挑战，制约了系统效能。本文提出Synapse框架，通过训练共享的全局工具使用知识模型应对该问题：固定LLM的客户端智能体在本地学习工具使用模式，通过协调器传输工件进行联邦聚合。系统持续更新并分发全局工具知识库，促使工具选择趋于稳定。Synapse采用模板化表示、嵌入检索与LLM重排、自适应掩码等技术，在限制信息泄露的同时保持实用性。该框架支持异构数据环境并量化性能提升，实验表明在多智能体LLM系统中，相比权重或提示共享方案，Synapse显著提升工具使用效能并降低通信开销。</div>
</details>
<details class="detail"><summary>Summary / 总结</summary>
<div class="mono">Collaborative learning among LLM-based agents under federated learning faces challenges, including communication costs, heterogeneity in data, and tool-usage, limiting their effectiveness.</div>
</details>
</div></div>
    <details style="margin-top:16px" class="detail"><summary>History</summary>
      <div class="history-list"><a href="archive/20260205_0420.html">20260205_0420</a>
<a href="archive/20260202_0404.html">20260202_0404</a>
<a href="archive/20260201_1746.html">20260201_1746</a>
<a href="archive/20260201_1152.html">20260201_1152</a>
<a href="archive/20260201_0333.html">20260201_0333</a>
<a href="archive/20260131_0345.html">20260131_0345</a>
<a href="archive/20260130_0341.html">20260130_0341</a>
<a href="archive/20260129_0344.html">20260129_0344</a>
<a href="archive/20260128_0341.html">20260128_0341</a>
<a href="archive/20260127_0338.html">20260127_0338</a>
<a href="archive/20260126_0330.html">20260126_0330</a>
<a href="archive/20260125_0329.html">20260125_0329</a>
<a href="archive/20260124_0337.html">20260124_0337</a>
<a href="archive/20260123_0337.html">20260123_0337</a>
<a href="archive/20260122_0343.html">20260122_0343</a>
<a href="archive/20260121_0424.html">20260121_0424</a>
<a href="archive/20260119_0329.html">20260119_0329</a>
<a href="archive/20260118_0327.html">20260118_0327</a>
<a href="archive/20260117_0332.html">20260117_0332</a>
<a href="archive/20260116_0339.html">20260116_0339</a>
<a href="archive/20260115_0334.html">20260115_0334</a>
<a href="archive/20260114_0333.html">20260114_0333</a>
<a href="archive/20260113_0334.html">20260113_0334</a>
<a href="archive/20260112_0331.html">20260112_0331</a>
<a href="archive/20260111_0329.html">20260111_0329</a>
<a href="archive/20260110_0333.html">20260110_0333</a>
<a href="archive/20260109_0334.html">20260109_0334</a>
<a href="archive/20260108_0335.html">20260108_0335</a>
<a href="archive/20260107_0330.html">20260107_0330</a>
<a href="archive/20260106_0336.html">20260106_0336</a>
<a href="archive/20260105_0328.html">20260105_0328</a>
<a href="archive/20260104_0328.html">20260104_0328</a>
<a href="archive/20260103_0325.html">20260103_0325</a>
<a href="archive/20260102_0339.html">20260102_0339</a>
<a href="archive/20260101_0329.html">20260101_0329</a>
<a href="archive/20251231_0333.html">20251231_0333</a>
<a href="archive/20251230_0332.html">20251230_0332</a>
<a href="archive/20251229_0329.html">20251229_0329</a>
<a href="archive/20251228_0332.html">20251228_0332</a>
<a href="archive/20251227_0329.html">20251227_0329</a>
<a href="archive/20251226_0330.html">20251226_0330</a>
<a href="archive/20251225_0329.html">20251225_0329</a>
<a href="archive/20251224_0331.html">20251224_0331</a>
<a href="archive/20251223_0332.html">20251223_0332</a>
<a href="archive/20251222_0328.html">20251222_0328</a>
<a href="archive/20251221_0329.html">20251221_0329</a>
<a href="archive/20251220_0330.html">20251220_0330</a>
<a href="archive/20251219_0330.html">20251219_0330</a>
<a href="archive/20251218_0345.html">20251218_0345</a>
<a href="archive/20251217_0332.html">20251217_0332</a>
<a href="archive/20251216_0333.html">20251216_0333</a>
<a href="archive/20251215_0333.html">20251215_0333</a>
<a href="archive/20251214_0327.html">20251214_0327</a>
<a href="archive/20251212_0333.html">20251212_0333</a>
<a href="archive/20251211_0331.html">20251211_0331</a>
<a href="archive/20251210_0332.html">20251210_0332</a>
<a href="archive/20251209_0331.html">20251209_0331</a>
<a href="archive/20251208_0328.html">20251208_0328</a>
<a href="archive/20251207_0327.html">20251207_0327</a>
<a href="archive/20251206_0330.html">20251206_0330</a>
<a href="archive/20251205_0331.html">20251205_0331</a>
<a href="archive/20251204_0331.html">20251204_0331</a>
<a href="archive/20251203_0333.html">20251203_0333</a>
<a href="archive/20251202_0335.html">20251202_0335</a>
<a href="archive/20251201_0328.html">20251201_0328</a>
<a href="archive/20251130_0327.html">20251130_0327</a>
<a href="archive/20251129_0328.html">20251129_0328</a>
<a href="archive/20251128_0327.html">20251128_0327</a>
<a href="archive/20251127_0327.html">20251127_0327</a>
<a href="archive/20251126_0329.html">20251126_0329</a>
<a href="archive/20251125_0327.html">20251125_0327</a>
<a href="archive/20251124_0327.html">20251124_0327</a>
<a href="archive/20251123_0326.html">20251123_0326</a>
<a href="archive/20251122_0328.html">20251122_0328</a>
<a href="archive/20251121_0328.html">20251121_0328</a>
<a href="archive/20251120_0329.html">20251120_0329</a>
<a href="archive/20251119_0328.html">20251119_0328</a>
<a href="archive/20251118_0328.html">20251118_0328</a>
<a href="archive/20251117_0326.html">20251117_0326</a>
<a href="archive/20251116_0325.html">20251116_0325</a>
<a href="archive/20251115_0327.html">20251115_0327</a>
<a href="archive/20251114_0328.html">20251114_0328</a>
<a href="archive/20251113_0330.html">20251113_0330</a>
<a href="archive/20251112_0329.html">20251112_0329</a>
<a href="archive/20251111_0328.html">20251111_0328</a>
<a href="archive/20251110_0325.html">20251110_0325</a>
<a href="archive/20251109_0326.html">20251109_0326</a>
<a href="archive/20251108_0328.html">20251108_0328</a>
<a href="archive/20251107_0328.html">20251107_0328</a>
<a href="archive/20251106_0329.html">20251106_0329</a>
<a href="archive/20251105_0326.html">20251105_0326</a>
<a href="archive/20251104_0327.html">20251104_0327</a>
<a href="archive/20251103_0324.html">20251103_0324</a>
<a href="archive/20251102_0326.html">20251102_0326</a>
<a href="archive/20251101_0324.html">20251101_0324</a>
<a href="archive/20251031_0328.html">20251031_0328</a>
<a href="archive/20251030_0330.html">20251030_0330</a>
<a href="archive/20251029_0329.html">20251029_0329</a>
<a href="archive/20251028_0329.html">20251028_0329</a>
<a href="archive/20251027_0322.html">20251027_0322</a>
<a href="archive/20251026_0327.html">20251026_0327</a>
<a href="archive/20251025_0331.html">20251025_0331</a>
<a href="archive/20251024_0329.html">20251024_0329</a>
<a href="archive/20251023_0329.html">20251023_0329</a>
<a href="archive/20251022_0330.html">20251022_0330</a>
<a href="archive/20251021_0331.html">20251021_0331</a>
<a href="archive/20251020_0328.html">20251020_0328</a>
<a href="archive/20251019_0321.html">20251019_0321</a>
<a href="archive/20251018_0327.html">20251018_0327</a>
<a href="archive/20251017_0320.html">20251017_0320</a>
<a href="archive/20251016_0328.html">20251016_0328</a>
<a href="archive/20251015_0328.html">20251015_0328</a>
<a href="archive/20251014_0323.html">20251014_0323</a>
<a href="archive/20251011_0328.html">20251011_0328</a>
<a href="archive/20251010_0330.html">20251010_0330</a>
<a href="archive/20251009_0321.html">20251009_0321</a>
<a href="archive/20251008_0343.html">20251008_0343</a>
<a href="archive/20251007_0353.html">20251007_0353</a>
<a href="archive/20251006_0325.html">20251006_0325</a>
<a href="archive/20251005_0350.html">20251005_0350</a>
<a href="archive/20251004_0352.html">20251004_0352</a>
<a href="archive/20251003_0352.html">20251003_0352</a>
<a href="archive/20251002_0356.html">20251002_0356</a>
<a href="archive/20251001_0321.html">20251001_0321</a>
<a href="archive/20250925_0335.html">20250925_0335</a>
<a href="archive/20250924_0350.html">20250924_0350</a>
<a href="archive/20250923_0348.html">20250923_0348</a>
<a href="archive/20250922_0346.html">20250922_0346</a>
<a href="archive/20250921_0345.html">20250921_0345</a>
<a href="archive/20250920_0342.html">20250920_0342</a>
<a href="archive/20250919_0346.html">20250919_0346</a>
<a href="archive/20250918_0342.html">20250918_0342</a>
<a href="archive/20250917_0336.html">20250917_0336</a>
<a href="archive/20250916_0333.html">20250916_0333</a>
<a href="archive/20250915_0333.html">20250915_0333</a>
<a href="archive/20250914_0328.html">20250914_0328</a>
<a href="archive/20250913_0322.html">20250913_0322</a>
<a href="archive/20250912_0335.html">20250912_0335</a>
<a href="archive/20250911_0337.html">20250911_0337</a>
<a href="archive/20250910_0338.html">20250910_0338</a>
<a href="archive/20250909_0341.html">20250909_0341</a>
<a href="archive/20250908_0342.html">20250908_0342</a>
<a href="archive/20250907_0333.html">20250907_0333</a>
<a href="archive/20250906_0350.html">20250906_0350</a>
<a href="archive/20250905_0319.html">20250905_0319</a>
<a href="archive/20250904_0323.html">20250904_0323</a>
<a href="archive/20250903_0355.html">20250903_0355</a>
<a href="archive/20250902_0325.html">20250902_0325</a>
<a href="archive/20250901_0355.html">20250901_0355</a>
<a href="archive/20250831_0355.html">20250831_0355</a>
<a href="archive/20250830_0356.html">20250830_0356</a>
<a href="archive/20250829_0355.html">20250829_0355</a>
<a href="archive/20250828_0333.html">20250828_0333</a>
<a href="archive/20250827_1654.html">20250827_1654</a>
<a href="archive/20250827_1602.html">20250827_1602</a>
<a href="archive/20250827_1557.html">20250827_1557</a>
<a href="archive/20250827_0320.html">20250827_0320</a>
<a href="archive/20250826_0320.html">20250826_0320</a>
<a href="archive/20250825_1752.html">20250825_1752</a>
<a href="archive/20250825_1709.html">20250825_1709</a>
<a href="archive/20250825_1652.html">20250825_1652</a>
<a href="archive/20250825_1647.html">20250825_1647</a>
<a href="archive/20250825_1645.html">20250825_1645</a>
<a href="archive/20250825_1631.html">20250825_1631</a>
<a href="archive/20250825_1606.html">20250825_1606</a>
<a href="archive/20250825_1559.html">20250825_1559</a>
<a href="archive/20250825_1558.html">20250825_1558</a>
<a href="archive/20250825_1556.html">20250825_1556</a>
<a href="archive/20250825_1531.html">20250825_1531</a>
<a href="archive/20250825_1525.html">20250825_1525</a>
<a href="archive/20250825_1516.html">20250825_1516</a>
<a href="archive/20250825_1450.html">20250825_1450</a>
<a href="archive/20250825_1444.html">20250825_1444</a>
<a href="archive/20250825_1438.html">20250825_1438</a>
<a href="archive/20250825_1414.html">20250825_1414</a>
<a href="archive/20250825_1413.html">20250825_1413</a>
<a href="archive/20250825_1410.html">20250825_1410</a>
<a href="archive/20250825_1408.html">20250825_1408</a>
<a href="archive/20250825_1405.html">20250825_1405</a>
<a href="archive/20250825_1401.html">20250825_1401</a>
<a href="archive/20250825_1355.html">20250825_1355</a>
<a href="archive/20250825_1347.html">20250825_1347</a>
<a href="archive/20250825_1345.html">20250825_1345</a>
<a href="archive/20250825_1344.html">20250825_1344</a>
<a href="archive/20250825_1343.html">20250825_1343</a>
<a href="archive/20250825_1340.html">20250825_1340</a>
<a href="archive/20250825_1339.html">20250825_1339</a>
<a href="archive/20250825_1333.html">20250825_1333</a>
<a href="archive/20250825_1323.html">20250825_1323</a>
<a href="archive/20250825_1317.html">20250825_1317</a>
<a href="archive/20250825_1243.html">20250825_1243</a>
<a href="archive/20250824_0342.html">20250824_0342</a>
<a href="archive/20250823_0343.html">20250823_0343</a>
<a href="archive/20250823_0142.html">20250823_0142</a>
<a href="archive/20250822_2331.html">20250822_2331</a>
<a href="archive/20250822_2308.html">20250822_2308</a>
<a href="archive/20250822_2258.html">20250822_2258</a>
<a href="archive/20250822_2241.html">20250822_2241</a>
<a href="archive/20250822_2228.html">20250822_2228</a>
<a href="archive/20250822_2206.html">20250822_2206</a>
<a href="archive/20250822_2147.html">20250822_2147</a>
<a href="archive/20250822_2111.html">20250822_2111</a>
<a href="archive/20250822_1259.html">20250822_1259</a>
<a href="archive/20250822_1233.html">20250822_1233</a>
<a href="archive/20250822_1229.html">20250822_1229</a>
<a href="archive/20250822_1223.html">20250822_1223</a>
<a href="archive/20250822_1210.html">20250822_1210</a>
<a href="archive/20250822_1201.html">20250822_1201</a>
<a href="archive/20250822_1111.html">20250822_1111</a>
<a href="archive/20250822_1058.html">20250822_1058</a>
<a href="archive/20250822_1052.html">20250822_1052</a>
<a href="archive/20250822_1045.html">20250822_1045</a>
<a href="archive/20250822_0657.html">20250822_0657</a>
<a href="archive/20250822_0553.html">20250822_0553</a></div>
    </details>
    <div class="footer">Generated by arxiv-tracker</div>
  </div>
</body></html>
